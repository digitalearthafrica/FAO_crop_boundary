{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements fine-tuning of pre-trained [FracTAL ResUNet model](https://www.mdpi.com/2072-4292/13/11/2197) using monthly Planet RGB chunks and validation data. It also exports evaluation metrics and export fine-tuned model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from glob import glob\n",
    "# import imageio.v2 as imageio\n",
    "from osgeo import gdal, osr\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# add existing and decode modules to system path\n",
    "module_paths=['decode/FracTAL_ResUNet/models/semanticsegmentation',\n",
    "             'decode/FracTAL_ResUNet/nn/loss']\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from modules\n",
    "from FracTAL_ResUNet import FracTAL_ResUNet_cmtsk\n",
    "from datasets import *\n",
    "from ftnmt_loss import ftnmt_loss\n",
    "from evaluation import dice_coef,train_model_per_epoch,evaluate_model_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for model architecture or training\n",
    "n_filters = 32\n",
    "depth = 6\n",
    "n_classes = 1\n",
    "batch_size = 4\n",
    "ctx_name = 'cpu'\n",
    "gpu_id = 0\n",
    "epochs = 20\n",
    "lr = 0.0001\n",
    "lr_decay = None\n",
    "\n",
    "# other parameters\n",
    "country = 'Rwanda'\n",
    "str_month='08' # selected monthly data\n",
    "CPU_COUNT = cpu_count()\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(3857)\n",
    "\n",
    "# folder of input RGB chunk geotiffs\n",
    "input_folder='../0_Data_preparation/results/RGB_chunks'\n",
    "# folder of validation chunks\n",
    "groundtruth_folder='../0_Data_preparation/results/groundtruth'\n",
    "# folder to store evaluation metrics\n",
    "out_folder='metrics'\n",
    "\n",
    "# pre-trained model weights\n",
    "# trained_model='model_weights/Planet_france.params' # trained and fine-tuned on planet data of France\n",
    "trained_model='model_weights/Planet_pretrained-france_finetuned-india.params' # trained on planet data of France and fine-tuned on India\n",
    "# trained_model = 'model_weights/Airbus_pretrained-france_finetuned-india.params' # trained and fine-tuned on SPOT data of France\n",
    "\n",
    "# fine-tuned model weights\n",
    "finetuned_model=trained_model.replace('.params','_'+'month_'+str_month+'_finetuned.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(out_folder):\n",
    "    os.makedirs(out_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify RGB chunks with validation data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 groundtruth extent chunks\n",
      "Found 123 groundtruth boundary chunks\n"
     ]
    }
   ],
   "source": [
    "# extract chunk ids of validation data\n",
    "extent_names=glob(groundtruth_folder+'/'+country+'*crop_field_extent*.tif')\n",
    "bound_names=[extent_name.replace('extent','bound') for extent_name in extent_names]\n",
    "print('Found {} groundtruth extent chunks'.format(len(extent_names)))\n",
    "print('Found {} groundtruth boundary chunks'.format(len(extent_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 RGB images\n"
     ]
    }
   ],
   "source": [
    "# find Planet RGB chunks corresponding to validation chunks\n",
    "image_names=[]\n",
    "for extent_name in extent_names:\n",
    "    # extract id of validation chunk\n",
    "    chunk_id=os.path.basename(extent_name)[:-4].split('_')[-2:]\n",
    "    image_list=glob(os.path.join(input_folder,country+'*'+str_month+'_'+'_'.join(chunk_id)+'.tif'))\n",
    "    if len(image_list)<1:\n",
    "        print('no RGB found for chunk')\n",
    "    else:\n",
    "        for img in image_list:\n",
    "            image_names.append(img)\n",
    "print('Found {} RGB images'.format(len(image_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "train_dataset = Planet_Dataset_Masked(fold='train',image_names=image_names,extent_names=extent_names,\n",
    "                                     bound_names=bound_names, random_crop=True)\n",
    "\n",
    "# Loads data from a dataset and create mini batches\n",
    "train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size,num_workers=CPU_COUNT) # might encounter 'connection refused' issue\n",
    "# train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n"
     ]
    }
   ],
   "source": [
    "# Set MXNet ctx\n",
    "if ctx_name == 'cpu':\n",
    "    ctx = mx.cpu()\n",
    "elif ctx_name == 'gpu':\n",
    "    ctx = mx.gpu(gpu_id)\n",
    "\n",
    "# initialise model\n",
    "model = FracTAL_ResUNet_cmtsk(nfilters_init=n_filters, depth=depth, NClasses=n_classes)\n",
    "\n",
    "# load pre-trained model parameters\n",
    "model.load_parameters(trained_model, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function, set optimisation method and initialise assessment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['batch_size'] = batch_size\n",
    "args['ctx_name'] = ctx_name\n",
    "\n",
    "# define optimisation method\n",
    "if lr_decay:\n",
    "    schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "    adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "else:\n",
    "    adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "\n",
    "# define loss function\n",
    "loss_function = ftnmt_loss(depth=5)\n",
    "\n",
    "# apply parameters optimisation on model parameters\n",
    "trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "# containers for metrics to log\n",
    "train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                 'train_mcc': [], 'train_dice': []}\n",
    "val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "               'val_mcc': [], 'val_dice': []}\n",
    "best_mcc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize metrics\n",
    "cumulative_loss = 0\n",
    "accuracy = mx.metric.Accuracy()\n",
    "f1 = mx.metric.F1()\n",
    "mcc = mx.metric.MCC()\n",
    "dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "if args['ctx_name'] == 'cpu':\n",
    "    ctx = mx.cpu()\n",
    "else:\n",
    "    ctx = mx.gpu(args['gpu'])\n",
    "\n",
    "epoch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune model parameters and export evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:  29%|██▉       | 9/31 [03:30<08:26, 23.03s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialise best mcc \n",
    "best_mcc=0.0\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model_per_epoch(\n",
    "    train_dataloader, model, loss_function, trainer, epoch, args)\n",
    "    \n",
    "    # training set metrics\n",
    "    train_loss_avg = train_loss / len(train_dataset) # average loss\n",
    "    train_metrics['train_loss'].append(train_loss_avg)\n",
    "    train_metrics['train_acc'].append(train_accuracy.get()[1]) # get accuracy value\n",
    "    train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "    train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "    train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "    \n",
    "    # validation set: use same dataset for now\n",
    "    val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model_per_epoch(\n",
    "        train_dataloader, model, loss_function, epoch, args)\n",
    "\n",
    "    # validation set metrics\n",
    "    val_dataset=train_dataset # set validation dataset the same as training dataset for now\n",
    "    val_loss_avg = val_loss / len(val_dataset)\n",
    "    val_metrics['val_loss'].append(val_loss_avg)\n",
    "    val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "    val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "    val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "    val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "    # print out evaluation scores for current epoch\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "        train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "    print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "        val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "    \n",
    "    # save model parameters based on best validation MCC metric: note this doesn't save model struture\n",
    "    if val_mcc.get()[1] > best_mcc:\n",
    "        model.save_parameters(finetuned_model)\n",
    "        best_mcc = val_mcc.get()[1]\n",
    "\n",
    "    # save metrics\n",
    "    metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "    out_metrics=os.path.join(out_folder,'_'.join(['metrics','month',str_month])+'.csv')\n",
    "    metrics.to_csv(out_metrics, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots()\n",
    "# use pyplot to visualise training curves\n",
    "ax.plot(np.arange(1, epoch+2),train_metrics['train_loss'],'g--',label='training_loss')\n",
    "ax.plot(np.arange(1, epoch+2),val_metrics['val_loss'],'r--',label='validation_loss')\n",
    "ax.plot(np.arange(1, epoch+2),train_metrics['train_acc'],'g',label='training_accuracy')\n",
    "ax.plot(np.arange(1, epoch+2),val_metrics['val_acc'],'r',label='validation_accuracy')\n",
    "ax.set_title('Training and Validation Curves')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('metrics')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dce633ad74794d18e744ff4895033c1217399eea64af39aea26b4d3f3272ece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
