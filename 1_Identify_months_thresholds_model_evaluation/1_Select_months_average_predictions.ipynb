{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook select the best performing months based on the model evaluation metrics generated from previous notebook and produce consensus predictions from the selected months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "from osgeo import gdal,osr\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# add existing and decode modules to system path\n",
    "module_paths=['decode/FracTAL_ResUNet/models/semanticsegmentation',\n",
    "             'decode/FracTAL_ResUNet/nn/loss']\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from modules\n",
    "from FracTAL_ResUNet import FracTAL_ResUNet_cmtsk\n",
    "from datasets import export_geotiff\n",
    "from datasets import *\n",
    "from evaluation import Calculate_IoUs,get_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for model architecture\n",
    "n_filters = 32\n",
    "depth = 6\n",
    "n_classes = 1\n",
    "batch_size = 4\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'cpu'\n",
    "gpu_id = 0\n",
    "\n",
    "# training metrics folder\n",
    "metrics_folder='metrics'\n",
    "# model parameters folder\n",
    "models_folder='model_weights'\n",
    "# folder of input RGB chunk geotiffs\n",
    "input_folder='../0_Data_preparation/results/RGB_chunks'\n",
    "# ground truth label folder\n",
    "groundtruth_folder='../0_Data_preparation/results/groundtruth'\n",
    "# output folder to store results of individual months\n",
    "out_folder='results_finetuned'\n",
    "# output folder to store averaged results\n",
    "out_folder_averaged='results_finetuned/averaged'\n",
    "\n",
    "# all candidate image months as strings\n",
    "# str_months=['02','04','06','08','10','12']\n",
    "str_months=['03','04','08','10','11','12']\n",
    "\n",
    "str_year='2021'\n",
    "# country = 'Mozambique'\n",
    "country = 'Rwanda'\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(3857)\n",
    "prj=srs.ExportToWkt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MXNet ctx\n",
    "if ctx_name == 'cpu':\n",
    "    ctx = mx.cpu()\n",
    "elif ctx_name == 'gpu':\n",
    "    ctx = mx.gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "if not os.path.isdir(out_folder_averaged):\n",
    "    os.makedirs(out_folder_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training/validation metrics and select months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected months and mcc values:  {'10': 0.4328345198321451, '04': 0.3520785675064072, '12': 0.3209426847704443}\n"
     ]
    }
   ],
   "source": [
    "val_mccs={}\n",
    "for str_month in str_months:\n",
    "    metrics_month_file=os.path.join(metrics_folder,'metrics_month_'+str_month+'.csv')\n",
    "    if os.path.exists(metrics_month_file):\n",
    "        metrics_month=pd.read_csv(metrics_month_file)\n",
    "        val_mccs[str_month]=metrics_month['val_mcc'].max()\n",
    "highest_mccs=dict(Counter(val_mccs).most_common(np.min([3,len(val_mccs)])))\n",
    "print('selected months and mcc values: ',highest_mccs)\n",
    "selected_months=list(highest_mccs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify RGB chunks with validation data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 groundtruth extent chunks\n",
      "Found 123 groundtruth boundary chunks\n"
     ]
    }
   ],
   "source": [
    "# extract chunk ids of validation data\n",
    "extent_names=glob(groundtruth_folder+'/'+country+'*crop_field_extent*.tif')\n",
    "bound_names=[extent_name.replace('extent','bound') for extent_name in extent_names]\n",
    "print('Found {} groundtruth extent chunks'.format(len(extent_names)))\n",
    "print('Found {} groundtruth boundary chunks'.format(len(bound_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 RGB images for month 10\n",
      "Found 123 RGB images for month 04\n",
      "Found 123 RGB images for month 12\n"
     ]
    }
   ],
   "source": [
    "# find Planet RGB chunks corresponding to validation chunks\n",
    "image_names_months=[]\n",
    "for str_month in selected_months:\n",
    "    image_names=[]\n",
    "    for extent_name in extent_names:\n",
    "        # extract id of validation chunk\n",
    "        chunk_id=os.path.basename(extent_name)[:-4].split('_')[-2:]\n",
    "        image_list=glob(os.path.join(input_folder,country+'*'+str_month+'_'+'_'.join(chunk_id)+'.tif'))\n",
    "        if len(image_list)<1:\n",
    "            print('no RGB found for chunk')\n",
    "        else:\n",
    "            for img in image_list:\n",
    "                image_names.append(img)\n",
    "    print('Found {} RGB images for month {}'.format(len(image_names),str_month))\n",
    "    image_names_months.append(image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders=[]\n",
    "for i in range(len(selected_months)):\n",
    "    # Define dataset\n",
    "    test_dataset = Planet_Dataset_No_labels(image_names=image_names_months[i])\n",
    "    # Loads data from a dataset and create mini batches\n",
    "    # test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size,num_workers=CPU_COUNT) # might encounter 'connection refused' issue\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size,num_workers=1)\n",
    "    data_loaders.append(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n",
      "model weights for month 10: model_weights/Planet_pretrained-france_finetuned-india_month_10_finetuned.params\n",
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n",
      "model weights for month 04: model_weights/Planet_pretrained-france_finetuned-india_month_04_finetuned.params\n",
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n",
      "model weights for month 12: model_weights/Planet_pretrained-france_finetuned-india_month_12_finetuned.params\n"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "for str_month in selected_months:\n",
    "    # initialise model\n",
    "    model = FracTAL_ResUNet_cmtsk(nfilters_init=n_filters, depth=depth, NClasses=n_classes)\n",
    "    # search for model weights parameters\n",
    "    model_weights=glob(os.path.join(models_folder,'*month_'+str_month+'_finetuned.params'))\n",
    "    if len(model_weights)>0:\n",
    "        print('model weights for month {}: {}'.format(str_month,model_weights[0]))\n",
    "        # load pre-trained model parameters\n",
    "        model.load_parameters(model_weights[0], ctx=ctx)\n",
    "        models.append(model)\n",
    "    else:\n",
    "        print('cant find model weights for month {}'.format(str_month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions for each selected month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using model parameters for month  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [05:02<00:00,  9.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using model parameters for month  04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [05:02<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using model parameters for month  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [05:02<00:00,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 52s, sys: 3min 32s, total: 30min 24s\n",
      "Wall time: 15min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outnames_extent={str_month:[] for str_month in selected_months}\n",
    "outnames_bound={str_month:[] for str_month in selected_months}\n",
    "# run model\n",
    "for m, (model, dataloader) in enumerate(zip(models,data_loaders)):\n",
    "    print('Predicting using model parameters for month ',selected_months[m])\n",
    "    for batch_i, img_data in enumerate(tqdm(dataloader)):\n",
    "        # extract batch data\n",
    "        imgs,id_dates,geotrans=img_data\n",
    "        bt_size=id_dates.asnumpy().shape[0]\n",
    "\n",
    "        # make a copy if the variable currently lives in the wrong context\n",
    "        imgs = imgs.as_in_context(ctx)\n",
    "\n",
    "        # predicted outputs: field extent probability, field boundary probability and distance to boundary\n",
    "        logits, bound, dist = model(imgs)\n",
    "\n",
    "        # export predictions for all images in the batch\n",
    "        for i in range(bt_size):\n",
    "            id_date=id_dates[i,:].asnumpy().astype(int)\n",
    "            str_id_date=[str(id_date[0])] # year\n",
    "            str_id_date.append(str(id_date[1]).zfill(2)) # month\n",
    "            str_id_date.extend([str(s).zfill(3) for s in id_date[2:]]) # zfill row and col ids so that output files also have uniform file name length\n",
    "            gt=geotrans[i,:].asnumpy()\n",
    "\n",
    "            outname_extent=os.path.join(out_folder,country+'_extent_prob_'+'_'.join(str_id_date)+'.tif')\n",
    "            export_geotiff(outname_extent,logits[i,:,:].asnumpy().squeeze(),gt,prj,gdal.GDT_Float32)\n",
    "\n",
    "            outname_bound=os.path.join(out_folder,country+'_bound_prob_'+'_'.join(str_id_date)+'.tif')\n",
    "            export_geotiff(outname_bound,bound[i,:,:].asnumpy().squeeze(),gt,prj,gdal.GDT_Float32)\n",
    "\n",
    "    #         outname_dist=os.path.join(out_folder,country+'_distance'+'_'.join(str_id_date)+'.tif')\n",
    "    #         export_geotiff(outname_dist,dist[i,:,:].asnumpy().squeeze(),gt,prj,gdal.GDT_Float32)\n",
    "            outnames_extent[selected_months[m]].append(outname_extent)\n",
    "            outnames_bound[selected_months[m]].append(outname_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate consensus results of selected months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_mean_acc=[]\n",
    "averaged_mean_f1=[]\n",
    "averaged_mean_mcc=[]\n",
    "# loop through all chunks and average over months\n",
    "for i in range(len(extent_names)):\n",
    "    list_extents=[]\n",
    "    list_bounds=[]\n",
    "    for str_month in selected_months:\n",
    "        # read in field extent probability geotiff and metadata\n",
    "        extent_prob_predicted_file=outnames_extent[str_month][i]\n",
    "        ds_extent = gdal.Open(extent_prob_predicted_file)\n",
    "        geotrans=ds_extent.GetGeoTransform()\n",
    "        proj=ds_extent.GetProjection()\n",
    "        np_extent = ds_extent.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "        # read in boundary probability\n",
    "        bound_prob_predicted_file=outnames_bound[str_month][i]\n",
    "        ds_bound=gdal.Open(bound_prob_predicted_file)\n",
    "        np_bound = ds_bound.GetRasterBand(1).ReadAsArray()\n",
    "        \n",
    "        list_extents.append(np_extent)\n",
    "        list_bounds.append(np_bound)\n",
    "        # release memory\n",
    "        ds_extent=None\n",
    "        ds_bound=None\n",
    "    # calculate averages\n",
    "    extent_average=np.mean(list_extents,axis=0)\n",
    "    bound_average=np.mean(list_bounds,axis=0)\n",
    "    \n",
    "    # find corresponding groundtruth extents and boundary probability files\n",
    "    chunk_id='_'.join(os.path.basename(outnames_extent[str_month][i])[:-4].split('_')[-2:])\n",
    "    fn_prefix='_'.join([country,'*extent',chunk_id])+'.tif'\n",
    "    file_extent_true=glob(groundtruth_folder+'/'+fn_prefix)[0]\n",
    "    \n",
    "    # read in ground truth extent and boundary file\n",
    "    extent_true=imageio.imread(file_extent_true)\n",
    "    boundary_true=imageio.imread(file_extent_true.replace('extent','bound'))\n",
    "    \n",
    "    # calculate evaluation scores\n",
    "    accuracy,f1,mcc=get_accuracy_scores(extent_true,boundary_true,extent_average)\n",
    "    averaged_mean_acc.append(accuracy.get()[1])\n",
    "    averaged_mean_f1.append(f1.get()[1])\n",
    "    averaged_mean_mcc.append(mcc.get()[1])\n",
    "    \n",
    "    # export as geotiffs\n",
    "    outname_extent='_'.join([country,'average_extent_prob',str_year,'_'.join(selected_months),chunk_id])+'.tif'\n",
    "    outname_extent=os.path.join(out_folder_averaged,outname_extent)\n",
    "    export_geotiff(outname_extent,extent_average,geotrans,proj,gdal.GDT_Float32)\n",
    "\n",
    "    outname_bound='_'.join([country,'average_bound_prob',str_year,'_'.join(selected_months),chunk_id])+'.tif'\n",
    "    outname_bound=os.path.join(out_folder_averaged,outname_bound)\n",
    "    export_geotiff(outname_bound,bound_average,geotrans,proj,gdal.GDT_Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy of months-averaged predictions:  0.8417284560266809\n",
      "mean F1 score of months-averaged predictions:  0.90329837455408\n",
      "mean MCC of months-averaged predictions:  0.38680642543516536\n"
     ]
    }
   ],
   "source": [
    "print('mean accuracy of months-averaged predictions: ',np.mean(averaged_mean_acc))\n",
    "print('mean F1 score of months-averaged predictions: ',np.mean(averaged_mean_f1))\n",
    "print('mean MCC of months-averaged predictions: ',np.mean(averaged_mean_mcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dce633ad74794d18e744ff4895033c1217399eea64af39aea26b4d3f3272ece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
