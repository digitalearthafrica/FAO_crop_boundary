{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook finetunes field extent and boundary probability thresholds used for crop field instance segmentation. It implements a grid search to find the thresholds with highest IoUs using validation dataset, then implements crop field instance segmentation using the finetuned thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting higra\n",
      "  Downloading higra-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from higra) (1.23.5)\n",
      "Installing collected packages: higra\n",
      "Successfully installed higra-0.6.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip install higra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import higra as hg\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from skimage import measure\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from modules\n",
    "from datasets import export_geotiff\n",
    "from instance_segment import InstSegm\n",
    "from evaluation import Calculate_IoUs,get_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input folder for predictions\n",
    "input_folder='results/averaged'\n",
    "\n",
    "# input folder for groundtruth field exent chunks\n",
    "groundtruth_folder='../0_Data_preparation/results/groundtruth'\n",
    "\n",
    "# output folder to store instance segmentation results\n",
    "out_folder=input_folder\n",
    "\n",
    "# hyperparameter values: extent and boundary probability thresholds\n",
    "t_exts = np.linspace(0.1, 0.6, 6)\n",
    "t_bounds = np.linspace(0.0, 0.3, 4)\n",
    "\n",
    "# proportion of randomly selected validation samples to be used for thresholds fine-tuning\n",
    "pct_samples=0.8\n",
    "\n",
    "# whether to save results as a pandas dataframe\n",
    "save_grd_search=True\n",
    "\n",
    "# country = 'Mozambique'\n",
    "country = 'Rwanda'\n",
    "str_year='2021'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify predicted exent and boundary chunks and groundtruth field extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 field exent probability images\n"
     ]
    }
   ],
   "source": [
    "files_extent_predictions=glob(input_folder+'/'+country+'*average_extent_prob*.tif')\n",
    "print('Found {} field exent probability images'.format(len(files_extent_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 true field extent images\n"
     ]
    }
   ],
   "source": [
    "files_extent_true=glob(groundtruth_folder+'/'+country+'*crop_field_extent*.tif')\n",
    "print('Found {} true field extent images'.format(len(files_extent_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply instance segmentation for a proportion of samples and find thresholds with highest IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select a proportion of validation chunks\n",
    "- Grid search boundary and extent probability thresholds and calculate IoUs\n",
    "- Find the thresholds with highest median IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly samples from the validation dataset\n",
    "n_samples=len(files_extent_true)\n",
    "random_inds=np.random.choice(n_samples, int(pct_samples*n_samples), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation thresholds with highest mIoU:\n",
      " t_ext      0.400000\n",
      "t_bound    0.300000\n",
      "mIoU       0.502924\n",
      "IoU_50     0.501644\n",
      "Name: 15, dtype: float64\n",
      "CPU times: user 8min 34s, sys: 6.51 s, total: 8min 41s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mIoUs = [] # median IoU\n",
    "IoU_50s = [] # fraction of fields with >half overlap\n",
    "for t_ext in t_exts:\n",
    "    print('applying threshold of exent: ',t_ext)\n",
    "    for t_bound in t_bounds:\n",
    "        print('applying threshold of boundary: ',t_bound)\n",
    "        best_IoUs_all=[]\n",
    "        # loop through all selected files\n",
    "        for idx in random_inds:\n",
    "            \n",
    "            # groundtruth exent\n",
    "            file_extent_true=files_extent_true[idx]\n",
    "            \n",
    "            # get chunk id\n",
    "            chunk_id='_'.join(os.path.basename(file_extent_true)[:-4].split('_')[-2:])\n",
    "            fn_prefix='_'.join([country,'average_extent_prob',str_year,'*',chunk_id])+'.tif'\n",
    "            \n",
    "            # find corresponding extent prediction chunk\n",
    "            list_files=glob(input_folder+'/'+fn_prefix)\n",
    "            \n",
    "            if len(list_files)>0:\n",
    "                file_exent_prediction=list_files[0]\n",
    "                file_bound_prediction=file_exent_prediction.replace('extent','bound')\n",
    "\n",
    "                # read in files\n",
    "                extent_true=imageio.imread(file_extent_true)\n",
    "                extent_prob=imageio.imread(file_exent_prediction)\n",
    "                bound_prob=imageio.imread(file_bound_prediction)\n",
    "\n",
    "                # do segmentation using current thresholds\n",
    "                instances_predicted=InstSegm(extent_prob, bound_prob, t_ext=t_ext, t_bound=t_bound)\n",
    "                # label connected regions, non-field (-1) will be labelled as 0\n",
    "                instances_predicted= measure.label(instances_predicted, background=-1,return_num=False)\n",
    "\n",
    "                # label groundtruth crop fields\n",
    "                instances_true= measure.label(extent_true, background=-1,return_num=False)\n",
    "\n",
    "                # calculate IoU\n",
    "                best_IoUs, field_sizes=Calculate_IoUs(instances_true, instances_predicted, plot=False)\n",
    "                best_IoUs_all.extend(best_IoUs)\n",
    "        mIoUs.append(np.median(best_IoUs_all))\n",
    "        IoU_50s.append(np.sum(np.array(best_IoUs_all) > 0.5) / len(best_IoUs_all))\n",
    "\n",
    "hp_df = pd.DataFrame({\n",
    "    't_ext': np.repeat(t_exts, len(t_bounds)),\n",
    "    't_bound': np.tile(t_bounds, len(t_exts)),\n",
    "    'mIoU': mIoUs,\n",
    "    'IoU_50': IoU_50s\n",
    "})\n",
    "# save results as a pandas dataframe\n",
    "if save_grd_search:\n",
    "    hp_df.to_csv(os.path.join(out_folder,country+'_grid_search_thresholds.csv'))\n",
    "print('segmentation thresholds with highest mIoU:\\n',hp_df.iloc[hp_df['mIoU'].idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do instance segmentation for all samples, evaluate and export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median IoU using the best threholds:  0.5026737967914439\n",
      "IoU_50 using the best threholds:  0.5013550135501355\n",
      "CPU times: user 20.9 s, sys: 1.06 s, total: 21.9 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use best thresholds\n",
    "t_ext_best=hp_df.iloc[hp_df['mIoU'].idxmax()]['t_ext']\n",
    "t_bnd_best=hp_df.iloc[hp_df['mIoU'].idxmax()]['t_bound']\n",
    "# list of IoUs\n",
    "best_IoUs_all=[]\n",
    "# loop through all validation chunks\n",
    "for file_extent_true in files_extent_true:\n",
    "    # get chunk id\n",
    "    chunk_id='_'.join(os.path.basename(file_extent_true)[:-4].split('_')[-2:])\n",
    "    fn_prefix='_'.join([country,'average_extent_prob',str_year,'*',chunk_id])+'.tif'\n",
    "\n",
    "    # find corresponding extent prediction chunk\n",
    "    list_files=glob(input_folder+'/'+fn_prefix)\n",
    "\n",
    "    if len(list_files)>0:\n",
    "        file_exent_prediction=list_files[0]\n",
    "        \n",
    "        # extract geo information using gdal\n",
    "        ds = gdal.Open(file_exent_prediction)\n",
    "        geotrans=ds.GetGeoTransform()\n",
    "        proj=ds.GetProjection()\n",
    "        ds=None\n",
    "        # corresponding boundary chunk file\n",
    "        file_bound_prediction=file_exent_prediction.replace('extent','bound')\n",
    "\n",
    "        # read in arrays\n",
    "        extent_true=imageio.imread(file_extent_true)\n",
    "        extent_prob=imageio.imread(file_exent_prediction)\n",
    "        bound_prob=imageio.imread(file_bound_prediction)\n",
    "\n",
    "        # do segmentation using selected thresholds\n",
    "        instances_predicted=InstSegm(extent_prob, bound_prob, t_ext=t_ext_best, t_bound=t_bnd_best)\n",
    "        # label connected regions, non-field (-1) will be labelled as 0\n",
    "        instances_predicted= measure.label(instances_predicted, background=-1,return_num=False)\n",
    "\n",
    "        # label groundtruth crop fields\n",
    "        instances_true= measure.label(extent_true, background=-1,return_num=False)\n",
    "\n",
    "        # calculate IoU\n",
    "        best_IoUs, field_sizes=Calculate_IoUs(instances_true, instances_predicted, plot=False)\n",
    "        best_IoUs_all.extend(best_IoUs)\n",
    "        \n",
    "        # export instances as geotiff\n",
    "        outname=os.path.join(out_folder,os.path.basename(file_exent_prediction).replace('extent_prob','field_instance'))\n",
    "        export_geotiff(outname,instances_predicted,geotrans,proj,gdal.GDT_Int16)\n",
    "\n",
    "m_IoU=np.median(best_IoUs_all)\n",
    "IoU_50=np.sum(np.array(best_IoUs_all) > 0.5) / len(best_IoUs_all)\n",
    "print('median IoU using the best threholds: ',m_IoU)\n",
    "print('IoU_50 using the best threholds: ',IoU_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdb4f26bf53785eacf4a6dff741c826249ef78a5457602be9aaa7cff4587c238"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
